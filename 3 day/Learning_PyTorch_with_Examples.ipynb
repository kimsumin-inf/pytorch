{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor \n",
    "## 준비 운동 : numpy를 사용한 신경망 구성\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# -*- coding: utf-8 -*-"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(10000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 2723.0228651421253\n",
      "199 1817.5730331481852\n",
      "299 1214.756204251254\n",
      "399 813.2495448539973\n",
      "499 545.7053052303097\n",
      "599 367.34243656211174\n",
      "699 248.37443399983312\n",
      "799 168.98110698489702\n",
      "899 115.96870209968027\n",
      "999 80.55082271311235\n",
      "1099 56.873570526555895\n",
      "1199 41.03499402909029\n",
      "1299 30.432947268421067\n",
      "1399 23.33120670525244\n",
      "1499 18.570687346517943\n",
      "1599 15.377154997397025\n",
      "1699 13.233135227044366\n",
      "1799 11.792547442667766\n",
      "1899 10.823785567283188\n",
      "1999 10.171746761020028\n",
      "2099 9.732486998641882\n",
      "2199 9.436295125049224\n",
      "2299 9.236382435522241\n",
      "2399 9.101320059429197\n",
      "2499 9.00997901964166\n",
      "2599 8.948142481730734\n",
      "2699 8.906235977823387\n",
      "2799 8.877805552270864\n",
      "2899 8.85849659963829\n",
      "2999 8.845368122707624\n",
      "3099 8.836431856205717\n",
      "3199 8.83034225494934\n",
      "3299 8.826187778076585\n",
      "3399 8.823350242971095\n",
      "3499 8.821409959953424\n",
      "3599 8.820081684173037\n",
      "3699 8.819171330810482\n",
      "3799 8.818546692773712\n",
      "3899 8.81811761055977\n",
      "3999 8.817822528949435\n",
      "4099 8.817619373652509\n",
      "4199 8.817479352920444\n",
      "4299 8.817382741697052\n",
      "4399 8.817316010975478\n",
      "4499 8.81726987091195\n",
      "4599 8.817237935314235\n",
      "4699 8.817215809152401\n",
      "4799 8.817200464382921\n",
      "4899 8.817189812498468\n",
      "4999 8.817182411464188\n",
      "5099 8.817177264560971\n",
      "5199 8.817173682156334\n",
      "5299 8.817171186610562\n",
      "5399 8.817169446784733\n",
      "5499 8.817168232886367\n",
      "5599 8.817167385303499\n",
      "5699 8.817166793070886\n",
      "5799 8.817166378975987\n",
      "5899 8.817166089246896\n",
      "5999 8.817165886405565\n",
      "6099 8.817165744309982\n",
      "6199 8.817165644711565\n",
      "6299 8.817165574862564\n",
      "6399 8.817165525851685\n",
      "6499 8.817165491445373\n",
      "6599 8.817165467280397\n",
      "6699 8.817165450300823\n",
      "6799 8.817165438365084\n",
      "6899 8.81716542997156\n",
      "6999 8.817165424066795\n",
      "7099 8.817165419911372\n",
      "7199 8.81716541698605\n",
      "7299 8.81716541492604\n",
      "7399 8.817165413474944\n",
      "7499 8.81716541245249\n",
      "7599 8.817165411731866\n",
      "7699 8.817165411223845\n",
      "7799 8.81716541086562\n",
      "7899 8.817165410612963\n",
      "7999 8.81716541043473\n",
      "8099 8.817165410308972\n",
      "8199 8.817165410220223\n",
      "8299 8.817165410157578\n",
      "8399 8.817165410113358\n",
      "8499 8.817165410082131\n",
      "8599 8.817165410060085\n",
      "8699 8.817165410044513\n",
      "8799 8.817165410033514\n",
      "8899 8.817165410025742\n",
      "8999 8.817165410020253\n",
      "9099 8.817165410016374\n",
      "9199 8.817165410013633\n",
      "9299 8.817165410011697\n",
      "9399 8.817165410010329\n",
      "9499 8.817165410009359\n",
      "9599 8.817165410008677\n",
      "9699 8.81716541000819\n",
      "9799 8.817165410007853\n",
      "9899 8.817165410007608\n",
      "9999 8.817165410007439\n",
      "Result: y = -2.1401146910474035e-08 + 0.8567408411369923 x + 3.6920539559711897e-09 x^2 + -0.09333038876510703 x^3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch를 통한 신경망 구성\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import torch\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "dtype = torch.float\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-3.1416, -3.1384, -3.1353,  ...,  3.1353,  3.1384,  3.1416],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 4139.5185546875\n",
      "199 2819.70263671875\n",
      "299 1923.748291015625\n",
      "399 1314.858642578125\n",
      "499 900.5971069335938\n",
      "599 618.4334716796875\n",
      "699 426.0282287597656\n",
      "799 294.67730712890625\n",
      "899 204.90383911132812\n",
      "999 143.47671508789062\n",
      "1099 101.39714050292969\n",
      "1199 72.5380859375\n",
      "1299 52.723487854003906\n",
      "1399 39.10346221923828\n",
      "1499 29.730819702148438\n",
      "1599 23.273841857910156\n",
      "1699 18.820777893066406\n",
      "1799 15.746333122253418\n",
      "1899 13.62146282196045\n",
      "1999 12.151385307312012\n",
      "Result: y = -0.05185488611459732 + 0.8864530920982361 x + 0.008945831097662449 x^2 + -0.0975566953420639 x^3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AutoGrad를 사용한 역전파 단계"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "x = torch.linspace (-torch.pi, torch.pi,2000,device=device , dtype=dtype)\n",
    "y= torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype,requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # 3차 다항식에는 가중치 4개\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다.\n",
    "    # 모든 텐서들에 대한 손실의 변화도를 계산 \n",
    "    loss.backward()\n",
    "    # 경사 하강법을 사용하여 가중치를 직접 갱신\n",
    "    # torch.no_grad() : 가중치들이 requires_grad = True 지만 autograd에서는 이를 추적하지 않게하기 위해서\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * grad_a\n",
    "        b -= learning_rate * grad_b\n",
    "        c -= learning_rate * grad_c\n",
    "        d -= learning_rate * grad_d\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듬\n",
    "        a.grad =None\n",
    "        b.grad =None\n",
    "        c.grad =None\n",
    "        d.grad =None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 1923612.0\n",
      "199 1923637.5\n",
      "299 1923663.125\n",
      "399 1923689.0\n",
      "499 1923715.25\n",
      "599 1923741.625\n",
      "699 1923768.375\n",
      "799 1923795.25\n",
      "899 1923822.125\n",
      "999 1923849.375\n",
      "1099 1923877.0\n",
      "1199 1923904.75\n",
      "1299 1923932.625\n",
      "1399 1923960.75\n",
      "1499 1923989.25\n",
      "1599 1924017.75\n",
      "1699 1924046.5\n",
      "1799 1924075.75\n",
      "1899 1924105.0\n",
      "1999 1924134.5\n",
      "Result: y = 1.4728765487670898 + -0.6047661304473877 x + -1.081460952758789 x^2 + -2.5073742866516113 x^3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch nn Module  (keras 와 유사함)\n",
    "\n",
    "## 연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수를 자동으로 계산하는 매우 강력한 패러다임\n",
    "## 하지만 대규모 신경망에서는 autograd 만으로는 매우 저수준\n",
    "\n",
    "## 파이토치에서 nn 패키지는 layer와 거의 비슷한 Module의 집합을 정의\n",
    "## Module은 입력 텐서를 받고 출력 텐서를 계산하는 한편, 학습 가능한 매개변수를 갖는 텐서들을 내부 상태로 갖음\n",
    "## nn 패키지는 또한 신경망을 학습시킬 때 주로 사용하는 유용한 loss function 제공"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace (-torch.pi, torch.pi, 2000 )\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "p=torch.tensor([1,2,3])\n",
    "# unsqueeze : 1인 차원을 생성하는 함수\n",
    "# (2000, 1)의 shape를, p는 (3, )의 shape를 가지므로 이 경우 broadcast를 적용하여 (2000,3)의 shpae를 갖는 tensor를 얻음\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "print(\"xx: \", xx)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층으로 정의\n",
    "# nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 출력을 생성\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "# MSE ; Mean Squared Error 평균 제곱 오차 \n",
    "loss_fn  = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    loss = loss_fn(y_pred,y)\n",
    "\n",
    "    if t% 100 ==99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "    \n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "xx:  tensor([[ -3.1416,   9.8696, -31.0063],\n",
      "        [ -3.1384,   9.8499, -30.9133],\n",
      "        [ -3.1353,   9.8301, -30.8205],\n",
      "        ...,\n",
      "        [  3.1353,   9.8301,  30.8205],\n",
      "        [  3.1384,   9.8499,  30.9133],\n",
      "        [  3.1416,   9.8696,  31.0063]])\n",
      "99 nan\n",
      "199 nan\n",
      "299 nan\n",
      "399 nan\n",
      "499 nan\n",
      "599 nan\n",
      "699 nan\n",
      "799 nan\n",
      "899 nan\n",
      "999 nan\n",
      "1099 nan\n",
      "1199 nan\n",
      "1299 nan\n",
      "1399 nan\n",
      "1499 nan\n",
      "1599 nan\n",
      "1699 nan\n",
      "1799 nan\n",
      "1899 nan\n",
      "1999 nan\n",
      "Result: y = nan + nan x + nan x^2 + nan x^3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a721ca5c4bfd00a5c386d602c2b2aa589cdc03e44354db2ead110409b536b245"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}