{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "x = torch.ones(5)\n",
    "y= torch.zeros(3)\n",
    "#requires_grad : 변수들에 대한 손실 함수를 계산하기위해서 require_grad를 활성화 함.\n",
    "# 단 성능 상의 이유로 backward 연산은 한 번만 수행할 수 있음\n",
    "w = torch.randn(5,3,requires_grad=True)\n",
    "b= torch.randn(3, requires_grad=True)\n",
    "z= torch.matmul(x,w)+b\n",
    "# 역전파 : 매개변수는 주어진 매개변수에 대한 손실 함수의 변화도에 따라 조정\n",
    "# pytorch에 autograd라고 불리는 자동 미분 엔진이 내장되어 있슴\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\n",
    "print(z.grad_fn)\n",
    "print(loss.grad_fn)\n",
    "print(z)\n",
    "print(w)\n",
    "print(b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<AddBackward0 object at 0x7f97bc463fd0>\n",
      "<BinaryCrossEntropyWithLogitsBackward0 object at 0x7f97bc463d60>\n",
      "tensor([ 1.5258,  2.1986, -3.8160], grad_fn=<AddBackward0>)\n",
      "tensor([[ 1.2525,  1.0015, -2.2478],\n",
      "        [ 1.3078,  0.7054, -0.0174],\n",
      "        [-0.9001, -0.3161, -0.0475],\n",
      "        [ 0.2617,  0.9605, -1.8179],\n",
      "        [-0.8098, -0.7016, -0.2036]], requires_grad=True)\n",
      "tensor([0.4137, 0.5488, 0.5183], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# 변화도 계산하기\n",
    "loss.backward()\n",
    "#require_grad = True가 되어 있는 변수들\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2738, 0.3000, 0.0072],\n",
      "        [0.2738, 0.3000, 0.0072],\n",
      "        [0.2738, 0.3000, 0.0072],\n",
      "        [0.2738, 0.3000, 0.0072],\n",
      "        [0.2738, 0.3000, 0.0072]])\n",
      "tensor([0.2738, 0.3000, 0.0072])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "# torch.no_grad() 블록으로 둘러싸서 연산 추적을 멈출 수 있음 = detach()를 사용하면 됨\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "# 변화도 추적을 멈춰야 할 때\n",
    "# 신경망 일부 매개변수를 고정된 매개변수로 표시\n",
    "# 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때 연산 속도가 향상됨"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inp = torch.eye(5,requires_grad=True)\n",
    "out = (int+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph = True)\n",
    "print(inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph = True)\n",
    "print(inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph = True)\n",
    "print(inp.grad)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a721ca5c4bfd00a5c386d602c2b2aa589cdc03e44354db2ead110409b536b245"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}